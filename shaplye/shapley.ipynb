{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e183a93-9e35-4b3f-bb39-28c29abb1c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup Complete. Using Model: gemma-3-12b-it\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & API Setup\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Ensure your API Key is set in your environment variables\n",
    "# OR uncomment the line below and paste it directly (not recommended for sharing)\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_ACTUAL_API_KEY\"\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Use the STABLE model compatible with high volume (as of Dec 2025)\n",
    "MODEL_NAME = 'gemma-3-12b-it' \n",
    "\n",
    "print(f\"âœ… Setup Complete. Using Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0dd0224-3ced-4cd9-8492-7ade4d7c9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Optimized.\n",
      "   Original Features: 80+\n",
      "   Selected Features: 22\n",
      "   Sample Shape: (1, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/b36wl4j96vxgr9wbxq2zh6lm0000gn/T/ipykernel_24792/3035087702.py:33: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df_filtered[col] = pd.to_numeric(df_filtered[col], errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data & Filter for ONLY Important Features\n",
    "\n",
    "# 1. Define your whitelist\n",
    "IMPORTANT_FEATURES = [\n",
    "    \"Neighborhood\", \"Lot Area\", \"Lot Frontage\", \"Overall Quality\", \"Overall Condition\",\n",
    "    \"Year Built\", \"Year Remodeled/Added\", \"Above Ground Living Area (sqft)\",\n",
    "    \"First Floor Area (sqft)\", \"Total Basement Area (sqft)\", \"Basement Finished Area 1 (sqft)\",\n",
    "    \"Basement Full Bathrooms\", \"Full Bathrooms\", \"Bedrooms Above Ground\",\n",
    "    \"Kitchen Quality\", \"Heating Quality\", \"Central Air Conditioning\",\n",
    "    \"Exterior Quality\", \"Exterior Condition\", \"Basement Height Quality\",\n",
    "    \"Basement Condition\", \"Garage Capacity (Cars)\", \"Garage Area (sqft)\", \"Garage Type\"\n",
    "]\n",
    "\n",
    "# 2. Load Sample\n",
    "def load_and_filter(filepath):\n",
    "    # Parse file\n",
    "    data_dict = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if \":\" in line:\n",
    "                key, val = line.split(\":\", 1)\n",
    "                data_dict[key.strip()] = val.strip()\n",
    "    \n",
    "    df = pd.DataFrame([data_dict])\n",
    "    \n",
    "    # Filter columns immediately\n",
    "    # We use 'intersection' so code doesn't crash if a specific name is slightly misspelled\n",
    "    valid_cols = [c for c in IMPORTANT_FEATURES if c in df.columns]\n",
    "    df_filtered = df[valid_cols].copy()\n",
    "    \n",
    "    # Convert numbers\n",
    "    for col in df_filtered.columns:\n",
    "        df_filtered[col] = pd.to_numeric(df_filtered[col], errors='ignore')\n",
    "        \n",
    "    return df_filtered, valid_cols\n",
    "\n",
    "sample_df, final_columns = load_and_filter(\"random_sample.txt\")\n",
    "\n",
    "# 3. Load Background (AmesHousing.csv) and match columns\n",
    "full_df = pd.read_csv(\"AmesHousing.csv\")\n",
    "\n",
    "# Ensure background has exact same columns in exact same order\n",
    "background_pool = full_df[final_columns].copy()\n",
    "\n",
    "# Fix Missing Values (SHAP hates NaNs)\n",
    "# Numeric columns get Median, Text columns get \"Unknown\"\n",
    "for col in background_pool.columns:\n",
    "    if pd.api.types.is_numeric_dtype(background_pool[col]):\n",
    "        background_pool[col] = background_pool[col].fillna(background_pool[col].median())\n",
    "    else:\n",
    "        background_pool[col] = background_pool[col].fillna(\"Unknown\")\n",
    "\n",
    "print(f\"âœ… Data Optimized.\")\n",
    "print(f\"   Original Features: 80+\")\n",
    "print(f\"   Selected Features: {len(final_columns)}\")\n",
    "print(f\"   Sample Shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bd22399-4e27-428c-ae62-3963d6fcf74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: API Bridge (30s Delay Logic)\n",
    "\n",
    "def call_gemini_api(prompt):\n",
    "    \"\"\"\n",
    "    API call with 30s delay to match your strict constraint.\n",
    "    \"\"\"\n",
    "    # Print a countdown so you know it's not frozen\n",
    "    print(\"   [Waiting 30s...]\", end=\"\\r\")\n",
    "    time.sleep(30) \n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel(MODEL_NAME)\n",
    "        \n",
    "        # Safety Settings\n",
    "        safety_settings = {\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "        \n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            candidate_count=1,\n",
    "            temperature=0.0, \n",
    "            max_output_tokens=500 \n",
    "        )\n",
    "        \n",
    "        full_prompt = (\n",
    "            \"Act as a Real Estate Calculator. \"\n",
    "            \"Predict the price for this house based on these features:\\n\"\n",
    "            f\"{prompt}\\n\"\n",
    "            \"Output ONLY the number.\"\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings\n",
    "        )\n",
    "        \n",
    "        if not response.parts: return 0.0\n",
    "        clean_number = re.sub(r\"[^\\d.]\", \"\", response.text)\n",
    "        return float(clean_number) if clean_number else 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n   Error: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def llm_predict_wrapper(data_numpy):\n",
    "    df_temp = pd.DataFrame(data_numpy, columns=sample_df.columns)\n",
    "    predictions = []\n",
    "    \n",
    "    # Progress Bar\n",
    "    total = len(df_temp)\n",
    "    for i, (_, row) in enumerate(df_temp.iterrows()):\n",
    "        print(f\"Processing {i+1}/{total}...\", end=\"\\r\")\n",
    "        \n",
    "        prompt_text = \"\"\n",
    "        for col, val in row.items():\n",
    "            prompt_text += f\"- {col}: {val}\\n\"\n",
    "        \n",
    "        price = call_gemini_api(prompt_text)\n",
    "        predictions.append(price)\n",
    "        \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d09ced84-e07c-457b-b9dd-d12230cf3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainer Ready (Using Single Median Background for Max Speed)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initialize Single-Row Background (Critical Optimization)\n",
    "\n",
    "# Strategy: Use ONE background sample (Median House).\n",
    "# If we used 10 background samples, your 25 mins would turn into 4 hours.\n",
    "# Using the median provides a perfectly valid \"Average Reference Point\".\n",
    "\n",
    "# 1. Calculate Median/Mode row\n",
    "median_row = {}\n",
    "for col in background_pool.columns:\n",
    "    if pd.api.types.is_numeric_dtype(background_pool[col]):\n",
    "        median_row[col] = background_pool[col].median()\n",
    "    else:\n",
    "        # For text, take the most common value (Mode)\n",
    "        median_row[col] = background_pool[col].mode()[0]\n",
    "\n",
    "background_summary = pd.DataFrame([median_row])\n",
    "\n",
    "# 2. Init Explainer\n",
    "explainer = shap.KernelExplainer(llm_predict_wrapper, background_summary)\n",
    "\n",
    "print(\"Explainer Ready (Using Single Median Background for Max Speed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf40a970-56d7-4977-aadc-e5c92e59901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculation for 22 features.\n",
      "Estimated Time: ~25 Minutes. Do not close this tab.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9939f68239f24a619d4db9b0296c072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Waiting 30s...]\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run Calculation\n",
    "\n",
    "# With 24 features, nsamples=50 is the mathematical minimum (2 * features + 2).\n",
    "# 50 samples * 1 background * 30 seconds = ~25 minutes.\n",
    "\n",
    "print(f\"Starting calculation for {len(sample_df.columns)} features.\")\n",
    "print(\"Estimated Time: ~25 Minutes. Do not close this tab.\")\n",
    "\n",
    "shap_values = explainer.shap_values(sample_df, nsamples=50)\n",
    "\n",
    "print(\"\\nDONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69a9e4c7-ecbe-447d-a127-7f677de0d2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS: Analysis Saved for House 0528326110\n",
      "   ðŸ“‚ shap_results/shap_values_0528326110.npy\n",
      "   ðŸ“‚ shap_results/features_0528326110.csv\n",
      "--------------------------------------------------\n",
      "NEXT STEP: Run this notebook again for the next house/sample.\n",
      "Once you have done 5 houses, run the 'Combiner' code below.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Data for Future Merging\n",
    "import json\n",
    "\n",
    "# 1. Extract Parcel ID for the filename (Safe Method)\n",
    "# Since 'Parcel ID' was filtered out of sample_df to save tokens, we read it from the file again.\n",
    "current_id = \"unknown_id\"\n",
    "try:\n",
    "    with open(\"random_sample.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"Parcel ID\" in line:\n",
    "                current_id = line.split(\":\")[1].strip()\n",
    "                break\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 2. Create Output Folder\n",
    "output_dir = \"shap_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3. Save SHAP Values (Numpy Binary Format)\n",
    "# This stores the output matrix (Price impact of each feature)\n",
    "shap_filename = os.path.join(output_dir, f\"shap_values_{current_id}.npy\")\n",
    "np.save(shap_filename, shap_values)\n",
    "\n",
    "# 4. Save Feature Data (CSV)\n",
    "# This stores the input values (What the house actually looked like)\n",
    "data_filename = os.path.join(output_dir, f\"features_{current_id}.csv\")\n",
    "sample_df.to_csv(data_filename, index=False)\n",
    "\n",
    "print(f\"âœ… SUCCESS: Analysis Saved for House {current_id}\")\n",
    "print(f\"   ðŸ“‚ {shap_filename}\")\n",
    "print(f\"   ðŸ“‚ {data_filename}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"NEXT STEP: Run this notebook again for the next house/sample.\")\n",
    "print(\"Once you have done 5 houses, run the 'Combiner' code below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd70747-eb29-4f5c-962e-c494aba40fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
